{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa34018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis of Weather type with other Parameters:\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "password_db='Bsa1986%40%21'\n",
    "database_name='bengaluru_traffic'\n",
    "host_port='127.0.0.1:3306'\n",
    "\n",
    "\n",
    "engine=create_engine(f'mysql+pymysql://root:{password_db}@{host_port}/{database_name}')\n",
    "query=\"\"\"\n",
    "SELECT \n",
    "    *\n",
    "FROM traffic_cleaned_geo;\n",
    "\"\"\"\n",
    "\n",
    "df=pd.read_sql_query(query,engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b915632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis of Overall city vs individual corridors of weather types with key metrics.\n",
    "\n",
    "weather_dummies=pd.get_dummies(df['weather'],prefix='weather_') #One Hot Encoding converting strings into binary equivalent.\n",
    "metrics=['traffic_volume','avg_speed','road_capacity_utilization','congestion_score','incident_reports','environmental_impact','ped_cycle_count','parking_usage','public_transport_usage']\n",
    "\n",
    "#Concat all relevant df for correlation analysis per corridor\n",
    "df_encoded=pd.concat([df[['area','road']], df[metrics], weather_dummies],axis=1)\n",
    "\n",
    "#Storing the base_metric categories for extraction of relevant df\n",
    "weather_list=[col for col in df_encoded.columns if 'weather_' in col]\n",
    "\n",
    "#Calculating the Overall City correlations:\n",
    "global_corr=df_encoded[weather_list+metrics].corr(method='spearman').round(3).loc[weather_list,metrics]\n",
    "\n",
    "corridors=df_encoded.groupby(['area','road'])\n",
    "results=[] \n",
    "\n",
    "#Iteration per corridor:\n",
    "for (area,road),group in corridors:\n",
    "    local_corr=group[weather_list+metrics].corr(method='spearman').round(3).loc[weather_list,metrics]\n",
    "\n",
    "    #Calculating any sizeable deviation\n",
    "    deviation=local_corr-global_corr\n",
    "    if abs(deviation).max().max()>0.12:\n",
    "        results.append({'area':area,'road':road,'deviation':deviation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To represent them into a Dataframe we need to convert them into 1-d from current 2-d shape using stack() function\n",
    "df_list=[]\n",
    "\n",
    "for item in results:\n",
    "    area=item['area']\n",
    "    road=item['road']\n",
    "\n",
    "    #Stackking of the results in matrix(2-d) form\n",
    "    matrix=item['deviation']\n",
    "\n",
    "    df_mid=matrix.stack().reset_index()\n",
    "    df_mid.columns=['weather_type','metric','deviation']\n",
    "    df_mid['area']=area\n",
    "    df_mid['road']=road\n",
    "\n",
    "    df_list.append(df_mid)\n",
    "\n",
    "#Final DataFrame using Concat Function\n",
    "df_final=pd.concat(df_list,ignore_index=True)\n",
    "\n",
    "#Extract deviations that are significant\n",
    "df_final=df_final[df_final['deviation'].abs()>0.15]\n",
    "df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3209198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation analysis of incident_reports of overall city and corridor-wise analysis.\n",
    "\n",
    "metrics=['traffic_volume','avg_speed','road_capacity_utilization','congestion_score','environmental_impact','ped_cycle_count','parking_usage','public_transport_usage']\n",
    "base_metric=['incident_reports']\n",
    "\n",
    "#Concat all relevant df for correlation analysis per corridor\n",
    "df_encoded=pd.concat([df[['area','road']], df[metrics+base_metric]],axis=1)\n",
    "\n",
    "#Calculating the Overall City correlations:\n",
    "global_corr=df_encoded[base_metric+metrics].corr(method='spearman').round(3).loc[base_metric,metrics]\n",
    "\n",
    "corridors=df_encoded.groupby(['area','road'])\n",
    "results=[] \n",
    "\n",
    "#Iteration per corridor:\n",
    "for (area,road),group in corridors:\n",
    "    local_corr=group[base_metric+metrics].corr(method='spearman').round(3).loc[base_metric,metrics]\n",
    "\n",
    "    #Calculating any sizeable deviation\n",
    "    deviation=local_corr-global_corr\n",
    "    if abs(deviation).max().max()>0.12:\n",
    "        results.append({'area':area,'road':road,'deviation':deviation})\n",
    "global_corr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To represent them into a Dataframe we need to convert them into 1-d from current 2-d shape using stack() function\n",
    "df_list=[]\n",
    "\n",
    "for item in results:\n",
    "    area=item['area']\n",
    "    road=item['road']\n",
    "\n",
    "    #Stackking of the results in matrix(2-d) form\n",
    "    matrix=item['deviation']\n",
    "\n",
    "    df_mid=matrix.stack().reset_index()\n",
    "    df_mid.columns=['incident_reports','metric','deviation']\n",
    "    df_mid['area']=area\n",
    "    df_mid['road']=road\n",
    "\n",
    "    df_list.append(df_mid)\n",
    "\n",
    "#Final DataFrame using Concat Function\n",
    "df_final=pd.concat(df_list,ignore_index=True)\n",
    "\n",
    "#Extract deviations that are significant\n",
    "df_final=df_final[df_final['deviation'].abs()>0.15]\n",
    "df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis of Mobility(ped_cycle_count) with other key metrics for the overall city and individual corridors and their sizeable deviation.\n",
    "\n",
    "base_metric=['ped_cycle_count']\n",
    "metrics=['traffic_volume','congestion_score','road_capacity_utilization','avg_speed','incident_reports','environmental_impact','parking_usage','public_transport_usage','traffic_signal_compliance']\n",
    "\n",
    "#Concatening required columns\n",
    "df_encoded=pd.concat([df[['area','road']], df[base_metric+metrics]],axis=1) #concatenation should happen horizontally\n",
    "\n",
    "#Calculating the Global Correlations\n",
    "global_corr=df_encoded[base_metric+metrics].corr(method='spearman').round(3).loc[base_metric,metrics]\n",
    "\n",
    "corridors=df_encoded.groupby(['area','road'])\n",
    "results=[]\n",
    "\n",
    "#iteration every corridor:\n",
    "for (area,road),group in corridors:\n",
    "    local_corr=group[base_metric+metrics].corr(method='spearman').round(3).loc[base_metric,metrics]\n",
    "\n",
    "    #Calculating significant deviations\n",
    "    deviation=local_corr-global_corr\n",
    "    if abs(deviation).max().max() > 0.15:\n",
    "        results.append({\n",
    "            'area':area,\n",
    "            'road':road,\n",
    "            'deviation':deviation})\n",
    "global_corr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a48528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion into a DataFrame by stacking the matrix results\n",
    "final_list=[]\n",
    "\n",
    "for item in results:\n",
    "    area=item['area']\n",
    "    road=item['road']\n",
    "    matrix=item['deviation']\n",
    "\n",
    "    #Stacking the matrix results\n",
    "    df_mid=matrix.stack().reset_index()\n",
    "    df_mid.columns=['Mobility','metric','deviation']\n",
    "    df_mid['area']=area\n",
    "    df_mid['road']=road\n",
    "\n",
    "    final_list.append(df_mid) #small dfs appending to a list\n",
    "\n",
    "#Final concatenation into single DataFrame:\n",
    "df_final=pd.concat(final_list)\n",
    "df_final=df_final[df_final['deviation'].abs() > 0.15]\n",
    "df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7577363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation of Environmental Impact City vs corridor\n",
    "base_metric=['environmental_impact']\n",
    "metrics=['traffic_volume','congestion_score','road_capacity_utilization','avg_speed','incident_reports','ped_cycle_count','parking_usage','public_transport_usage','traffic_signal_compliance']\n",
    "\n",
    "#Concatening required columns\n",
    "df_encoded=pd.concat([df[['area','road']], df[base_metric+metrics]],axis=1) \n",
    "\n",
    "#Calculating the Global Correlations\n",
    "global_corr=df_encoded[base_metric+metrics].corr(method='spearman').round(3).loc[base_metric,metrics]\n",
    "\n",
    "corridors=df_encoded.groupby(['area','road'])\n",
    "results=[]\n",
    "\n",
    "#iteration every corridor:\n",
    "for (area,road),group in corridors:\n",
    "    local_corr=group[base_metric+metrics].corr(method='spearman').round(3).loc[base_metric,metrics]\n",
    "\n",
    "    #Calculating significant deviations\n",
    "    deviation=local_corr-global_corr\n",
    "    if abs(deviation).max().max() > 0.15:\n",
    "        results.append({\n",
    "            'area':area,\n",
    "            'road':road,\n",
    "            'deviation':deviation})\n",
    "global_corr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df773ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion into a DataFrame by stacking the matrix results\n",
    "final_list=[]\n",
    "\n",
    "for item in results:\n",
    "    area=item['area']\n",
    "    road=item['road']\n",
    "    matrix=item['deviation']\n",
    "\n",
    "    #Stacking the matrix results\n",
    "    df_mid=matrix.stack().reset_index()\n",
    "    df_mid.columns=['Sustainability','metric','deviation']\n",
    "    df_mid['area']=area\n",
    "    df_mid['road']=road\n",
    "\n",
    "    final_list.append(df_mid)\n",
    "\n",
    "#Final concatenation into single DataFrame:\n",
    "df_final=pd.concat(final_list)\n",
    "df_final=df_final[df_final['deviation'].abs() > 0.15]\n",
    "df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Final Multi-Dimensional Corridor Priority score as the analytical summary and priorities of each corridor.\n",
    "#Weightage : 40% mean_congestion, 25% volatility(coeff_of_variation), 15% corridor sensitivity(Maverick Index), 10%(safety), 10%(sustainaibility)\n",
    "\n",
    "base_metric=['congestion_score']\n",
    "metrics=['traffic_volume','avg_speed','ped_cycle_count','incident_reports','environmental_impact']\n",
    "\n",
    "df_encoded=pd.concat([df[['area','road']], df[base_metric+metrics]],axis=1)\n",
    "\n",
    "#Calculating the global correlation:\n",
    "global_corr=df_encoded[base_metric+metrics].corr(method='spearman').round(3).loc[base_metric,metrics]\n",
    "\n",
    "#Calculating the corridor wise metrics\n",
    "corridors=df_encoded.groupby(['area','road'])\n",
    "results=[]\n",
    "\n",
    "for (area,road),group in corridors:\n",
    "    local_corr=group[base_metric+metrics].corr(method='spearman').round(3).loc[base_metric,metrics]\n",
    "    deviation=abs(local_corr-global_corr).round(3)\n",
    "   \n",
    "    \n",
    "    df_mid=deviation.stack().reset_index()\n",
    "    df_mid.columns=['base_metric','metric','deviation']\n",
    "    \n",
    "#     #Extracting key-metrics for each corridor for calculation of Multi-Dimensional KPI\n",
    "    maverick_index=df_mid['deviation'].mean()\n",
    "    m_cong=group['congestion_score'].mean()\n",
    "    v_cong=group['congestion_score'].std()\n",
    "    safety=group['incident_reports'].mean()\n",
    "    env=group['environmental_impact'].mean()\n",
    "\n",
    "#     #Now for each corridor creating a List of dictionary data type elements containing all essential metric data for each corridor\n",
    "    results.append({\n",
    "        'area':area,\n",
    "        'road':road,\n",
    "        'm_cong':m_cong,\n",
    "        'v_cong': v_cong/m_cong if m_cong>0 else 0,\n",
    "        'm_index':maverick_index,\n",
    "        'safety':safety,\n",
    "        'env':env\n",
    "    })\n",
    "\n",
    "df1=pd.DataFrame(results)\n",
    "df1\n",
    "\n",
    "#Normalize each of our metrics\n",
    "def normalize(s):\n",
    "    return (s-min(s))/(max(s)-min(s)) \n",
    "\n",
    "df1['n_cong']=normalize(df1['m_cong']).round(3)\n",
    "df1['n_vol']=normalize(df1['v_cong']).round(3)\n",
    "df1['n_mindex']=normalize(df1['m_index']).round(3)\n",
    "df1['n_safety']=normalize(df1['safety']).round(3)\n",
    "df1['n_env']=normalize(df1['env']).round(3)\n",
    "\n",
    "#Defining the Final Corridor KPI Matrix:\n",
    "df1['priority_score']= (df1['n_cong'] * 0.40 + df1['n_vol'] * 0.25 + df1['n_mindex'] * 0.15 + df1['n_safety'] * 0.10 + df1['n_env'] * 0.10) *100\n",
    "\n",
    "df_final=df1[['area','road','priority_score']]\n",
    "df_final=df_final.sort_values(by=['priority_score'],ascending=False).reset_index(drop=True)\n",
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
