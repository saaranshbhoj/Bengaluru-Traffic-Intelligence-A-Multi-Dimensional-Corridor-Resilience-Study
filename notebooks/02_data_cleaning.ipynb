{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths \n",
    "RAW_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "CLEAN_DIR = os.path.join(\"..\", \"data\", \"cleaned\")\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "FILE_NAME = \"Banglore_traffic_Dataset.csv\" \n",
    "raw_path = os.path.join(RAW_DIR, FILE_NAME)\n",
    "\n",
    "print(\"Raw file path:\", raw_path)\n",
    "\n",
    "if not os.path.exists(raw_path):\n",
    "    raise FileNotFoundError(f\"Raw file not found at: {raw_path}\")\n",
    "\n",
    "df = pd.read_csv(raw_path)\n",
    "print(\"Raw data loaded.\")\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4d78c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#– Defining column mapping (canonical names). We’ll map human-readable column names to internal/standard ones so later code is cleaner.\n",
    "COLUMN_MAP = {\n",
    "    \"Date\": \"timestamp\",\n",
    "    \"Area Name\": \"area\",\n",
    "    \"Road/Intersection Name\": \"road\",\n",
    "    \"Traffic Volume\": \"traffic_volume\",\n",
    "    \"Average Speed\": \"avg_speed\",\n",
    "    \"Travel Time Index\": \"travel_time_index\",\n",
    "    \"Congestion Level\": \"congestion_level\",\n",
    "    \"Road Capacity Utilization\": \"road_capacity_utilization\",\n",
    "    \"Incident Reports\": \"incident_reports\",\n",
    "    \"Environmental Impact\": \"environmental_impact\",\n",
    "    \"Public Transport Usage\": \"public_transport_usage\",\n",
    "    \"Traffic Signal Compliance\": \"traffic_signal_compliance\",\n",
    "    \"Parking Usage\": \"parking_usage\",\n",
    "    \"Pedestrian and Cyclist Count\": \"ped_cycle_count\",\n",
    "    \"Weather Conditions\": \"weather\",\n",
    "    \"Roadwork and Construction Activity\": \"roadwork\"\n",
    "}\n",
    "\n",
    "missing_col=[c for c in COLUMN_MAP.keys() if c not in df.columns] #dynamic list\n",
    "if len(missing_col) > 0:\n",
    "    print('following Columns are not in df :', missing_col)\n",
    "\n",
    "df_clean=df.rename(columns={a : b for a,b in COLUMN_MAP.items() if a in df.columns})  #{'old name':'new name'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373df6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before duplicate removal:\", df_clean.shape)\n",
    "dupes = df_clean.duplicated().sum() #Since subset parameter not provided it will look for duplicated rows inside the entire df.\n",
    "print(f'Found {dupes} fully duplicated rows.')\n",
    "\n",
    "if dupes > 0:\n",
    "    df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"Shape after duplicate removal:\", df_clean.shape)\n",
    "\n",
    "#Timestamp Parsing and edge cases handling\n",
    "if \"timestamp\" in df_clean.columns:\n",
    "    df_clean[\"timestamp\"] = pd.to_datetime(df_clean[\"timestamp\"], errors=\"coerce\") #error parameter is given to prevent the breaking of program flow.\n",
    "    n_bad = df_clean[\"timestamp\"].isna().sum()\n",
    "    print(f\"Failed to parse timestamp for {n_bad} rows.\")\n",
    "else:\n",
    "    raise ValueError(\"No 'Date'/'timestamp' column found. Check COLUMN_MAP definitions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with completely missing timestamp or location\n",
    "before = df_clean.shape[0]\n",
    "\n",
    "df_clean = df_clean.dropna(subset=[\"timestamp\", \"area\", \"road\"])\n",
    "after = df_clean.shape[0]\n",
    "print(f\"Dropped {before - after} rows with missing timestamp/area/road.\")\n",
    "\n",
    "# Derived date parts\n",
    "df_clean[\"date\"] = df_clean[\"timestamp\"].dt.date\n",
    "df_clean.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#– Standardize text columns\n",
    "text_cols=df_clean.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "for col in text_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = (\n",
    "            df_clean[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .replace({\"nan\": np.nan})  # if \"nan\" as string exists\n",
    "        )\n",
    "# For area & road\n",
    "        if col in [\"area\", \"road\"]:\n",
    "            df_clean[col] = df_clean[col].str.title()\n",
    "df_clean[text_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#– Numeric cleaning helpers\n",
    "def to_numeric(series, colname):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    print(f\"{colname}: {s.isna().sum()} values could not be converted to numeric and became NaN.\")\n",
    "    return s\n",
    "\n",
    "num_cols = [\n",
    "    \"traffic_volume\",\n",
    "    \"avg_speed\",\n",
    "    \"travel_time_index\",\n",
    "    \"road_capacity_utilization\",\n",
    "    \"incident_reports\",\n",
    "    \"environmental_impact\",\n",
    "    \"public_transport_usage\",\n",
    "    \"traffic_signal_compliance\",\n",
    "    \"parking_usage\",\n",
    "    \"ped_cycle_count\"\n",
    "]\n",
    "for col in num_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = to_numeric(df_clean[col], col) #calling the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#– Column-specific sanity rules\n",
    "\n",
    "# 1. Average speed: no negatives, cap at 120 km/h \n",
    "if \"avg_speed\" in df_clean.columns:\n",
    "    before_rows = len(df_clean)\n",
    "    neg_speed = (df_clean[\"avg_speed\"] < 0).sum()\n",
    "    print(f\"avg_speed: {neg_speed} negative values, setting to NaN.\")\n",
    "    df_clean.loc[df_clean[\"avg_speed\"] < 0, \"avg_speed\"] = np.nan \n",
    "    \n",
    "    # Cap extreme speeds\n",
    "    too_high = (df_clean[\"avg_speed\"] > 120).sum()\n",
    "    print(f\"avg_speed: {too_high} values >120 km/h, capping to 120.\")\n",
    "    df_clean.loc[df_clean[\"avg_speed\"] > 120, \"avg_speed\"] = 120.0\n",
    "\n",
    "# 2. Traffic volume: no negative counts, cap very high outliers\n",
    "if \"traffic_volume\" in df_clean.columns:\n",
    "    neg_vol = (df_clean[\"traffic_volume\"] < 0).sum()\n",
    "    print(f\"traffic_volume: {neg_vol} negative values, setting to NaN.\")\n",
    "    df_clean.loc[df_clean[\"traffic_volume\"] < 0, \"traffic_volume\"] = np.nan\n",
    "    \n",
    "    # Cap at 99th percentile to reduce influence of extreme outliers\n",
    "    q99 = df_clean[\"traffic_volume\"].quantile(0.99)\n",
    "    extreme = (df_clean[\"traffic_volume\"] > q99).sum()\n",
    "    print(f\"traffic_volume: {extreme} values >99th percentile ({q99:.0f}), capping.\")\n",
    "    df_clean.loc[df_clean[\"traffic_volume\"] > q99, \"traffic_volume\"] = q99\n",
    "\n",
    "# 3. Percentage columns: force into [0, 100]\n",
    "for perc_col in [\"road_capacity_utilization\", \"public_transport_usage\", \n",
    "                 \"traffic_signal_compliance\", \"parking_usage\"]:\n",
    "    if perc_col in df_clean.columns:\n",
    "        below0 = (df_clean[perc_col] < 0).sum()\n",
    "        above100 = (df_clean[perc_col] > 100).sum()\n",
    "        print(f\"{perc_col}: {below0} <0, {above100} >100; clipping to [0,100].\")\n",
    "        \n",
    "        df_clean.loc[df_clean[perc_col] < 0, perc_col] = 0\n",
    "        df_clean.loc[df_clean[perc_col] > 100, perc_col] = 100\n",
    "\n",
    "# 4. Incident and ped/cycle counts: no negatives\n",
    "for count_col in [\"incident_reports\", \"ped_cycle_count\"]:\n",
    "    if count_col in df_clean.columns:\n",
    "        neg = (df_clean[count_col] < 0).sum()\n",
    "        print(f\"{count_col}: {neg} negative values, setting to NaN.\")\n",
    "        df_clean.loc[df_clean[count_col] < 0, count_col] = np.nan\n",
    "\n",
    "# 5. Environmental impact\n",
    "if \"environmental_impact\" in df_clean.columns:\n",
    "    neg_env = (df_clean[\"environmental_impact\"] < 0).sum()\n",
    "    print(f\"environmental_impact: {neg_env} negative values, setting to NaN.\")\n",
    "    df_clean.loc[df_clean[\"environmental_impact\"] < 0, \"environmental_impact\"] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f806f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#– Create a combined congestion score (helper metric). It will help later in identifying key tarffic hotspots. Part of feature Engineering\n",
    "#-According to our analysis two columns ['travel_time_index','road_capacity_utilization'] that would be most contributing to the new metric.\n",
    "\n",
    "if all(c in df_clean.columns for c in [\"travel_time_index\", \"road_capacity_utilization\"]):\n",
    "    # Normalize travel_time_index to roughly 0–1 using min-max scaling\n",
    "    tti_min = df_clean[\"travel_time_index\"].min()\n",
    "    tti_max = df_clean[\"travel_time_index\"].max()\n",
    "    df_clean[\"tti_norm\"] = (df_clean[\"travel_time_index\"] - tti_min) / (tti_max - tti_min + 1e-6)  # 1e-6 is done to avoid division by zero \n",
    "    \n",
    "    # road_capacity_utilization is already in [0,100]; make [0,1]\n",
    "    df_clean[\"capacity_norm\"] = df_clean[\"road_capacity_utilization\"] / 100.0\n",
    "    \n",
    "    # Weighted score\n",
    "    df_clean[\"congestion_score\"] = (0.6 * df_clean[\"tti_norm\"] + 0.4 * df_clean[\"capacity_norm\"]).round(2)\n",
    "    \n",
    "    print(\"Created congestion_score.\")\n",
    "else:\n",
    "    print(\"⚠️ Could not create congestion_score; missing required columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf57e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geocoding- (lat/long) using geopy. We’ll geocode only unique (area, road) pairs to keep it efficient.\n",
    "\n",
    "try:\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.extra.rate_limiter import RateLimiter\n",
    "    \n",
    "    geolocator = Nominatim(user_agent=\"bengaluru_traffic_project\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1) \n",
    "    \n",
    "    print(\"✅ geopy imported, Nominatim geocoder ready.\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not import geopy or initialize geocoder:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9735536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the consistency of the dataset values:\n",
    "df_clean['road'] = df_clean['road'].str.title().str.replace(\" Rd\", \" Road\")\n",
    "df_clean['area'] = df_clean['area'].str.title()\n",
    "\n",
    "# Build unique location table\n",
    "location_cols = [\"area\", \"road\"]\n",
    "\n",
    "for col in location_cols:\n",
    "    if col not in df_clean.columns:\n",
    "        raise ValueError(f\"Missing required column for geocoding: {col}\")\n",
    "\n",
    "unique_locs = (\n",
    "    df_clean[location_cols]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Unique (area, road) pairs:\", len(unique_locs))\n",
    "unique_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29427fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lat/lon columns\n",
    "unique_locs[\"lat\"] = np.nan\n",
    "unique_locs[\"lon\"] = np.nan\n",
    "\n",
    "def geocode_row(row):\n",
    "    query = f\"{row['road']}, {row['area']}, Bengaluru, India\"\n",
    "    try:\n",
    "        location = geocode(query)\n",
    "        if location:\n",
    "            return pd.Series({\"lat\": location.latitude, \"lon\": location.longitude})\n",
    "    except Exception as e:\n",
    "        print(\"Geocoding error for\", query, \"→\", e)\n",
    "    return pd.Series({\"lat\": np.nan, \"lon\": np.nan})\n",
    "\n",
    "\n",
    "for idx in range(len(unique_locs)):\n",
    "    row = unique_locs.loc[idx, location_cols] #row-selection from unique_locs\n",
    "    \n",
    "    lat_lon = geocode_row(row) #calling the function and retrieving geo-coordinates\n",
    "\n",
    "    unique_locs.loc[idx, [\"lat\", \"lon\"]] = lat_lon #updating nan values to actual geopoints\n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"Geocoded {idx + 1}/{len(unique_locs)} locations...\")\n",
    "\n",
    "unique_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Through geo-encoding there were some corridors whose geo-coordinates were not extracted.\n",
    "missing_geo = unique_locs[\"lat\"].isna().sum()\n",
    "print(f\"Locations with missing geocode: {missing_geo} / {len(unique_locs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually filling nan geo-coordinates:\n",
    "manual = unique_locs[unique_locs[\"lat\"].isna()][[\"area\", \"road\"]].drop_duplicates() \n",
    "manual\n",
    "\n",
    "#Export them for manual lookup:\n",
    "manual.to_csv(\"manual_geo_lookup.csv\", index=False)\n",
    "print(\"✅ manual_geo_lookup.csv created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual = pd.read_csv(\"manual_geo_lookup.csv\")\n",
    "\n",
    "#merge with existing unique_locs:\n",
    "geo_data=unique_locs.merge(\n",
    "    manual, how='left',on=['road','area']\n",
    ")\n",
    "geo_data['lat_x']=geo_data['lat_x'].fillna(geo_data['lat_y'])\n",
    "geo_data['lon_x']=geo_data['lon_x'].fillna(geo_data['lon_y'])\n",
    "geo_data.drop(columns=['lat_y','lon_y'],inplace=True) #inplace=True for making changes to original DataFrame\n",
    "geo_data.rename(columns={'lat_x' : 'latitude','lon_x':'longitude'},inplace=True)\n",
    "geo_data\n",
    "\n",
    "#final check before merging with main df\n",
    "print(\"Remaining missing lat:\", geo_data[\"latitude\"].isna().sum())\n",
    "print(\"Remaining missing lon:\", geo_data[\"longitude\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final merging and preparation of data for SQL:\n",
    "df_final=df_clean.merge(\n",
    "    geo_data, how='left',on=['area','road']\n",
    ")\n",
    "\n",
    "#Final Error Check\n",
    "lat_err_count=df_final['latitude'].isna().sum()\n",
    "lon_err_count=df_final['longitude'].isna().sum()\n",
    "print(f'Total rows with missing latitude values:',lat_err_count)\n",
    "print(f'Total rows with missing longitude values:',lon_err_count)\n",
    "\n",
    "##Save the final data\n",
    "df_final.to_csv(\"../data/cleaned/traffic_cleaned_geo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
